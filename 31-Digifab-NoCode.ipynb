{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Volker Hoffmann, SINTEF <volker.hoffmann@sintef.no> <volker@cheleb.net>\n",
    "# Update: 12 December 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Scope\n",
    "\n",
    "In this notebook, we demonstrate how to\n",
    "\n",
    "1. Load some data from an sensor, and\n",
    "2. How to get some descriptive, visual, and predictive analytics\n",
    "\n",
    "As an example, we use an air temperature sensor which was placed around in our offices for a while.\n",
    "\n",
    "One fine day, something happened to the sensor. *Can you find out what?*\n",
    "\n",
    "### What's This? How does it work?\n",
    "\n",
    "What are working with right now is called an Jupyter (formerly IPython) notebook. It's a browser-based frontend to a Python process that is running on a some machine (can be remote or local). If you want to learn a bit more about this tool, have a look at https://jupyter.org/.\n",
    "\n",
    "The most important feature of the notebook is the use of cells that can be executed independently. In a cell, you can \n",
    "\n",
    "- Either write code (which you can execute by hitting `SHIFT+ENTER`).\n",
    "- Write markdown to tell a story.\n",
    "\n",
    "If you have used Mathematica before, this should feel very familiar.\n",
    "\n",
    "### Let's Start\n",
    "\n",
    "Before we start our data exploration, we need to get some generic Python preamble out of the way.\n",
    "\n",
    "The following cells load different libraries (called modules in Python) and set up our environment.\n",
    "\n",
    "A big part of working in Python is knowing the right libraries to get stuff done. In this notebook, we use\n",
    "\n",
    "- `Pandas` to handle our data\n",
    "- `Matplotlib` and `Seaborn` to make figures\n",
    "- `Scikit-learn` to compute clusters\n",
    "\n",
    "Other helpers are\n",
    "\n",
    "- `Glob` to get a list of files according to some pattern \n",
    "- `Numpy` to do some computations\n",
    "- `Warnings` to modify the verbosity of warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl; mpl.rcParams['figure.dpi'] = 96\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells demonstrate how to load files into Pandas.\n",
    "\n",
    "When we're done, we will have the data loaded into a structure called a Dataframe.\n",
    "\n",
    "The core function to load data from a CSV file is `pd.read_csv()`.\n",
    "\n",
    "You can think of it as an in-memory Excel table that you can use (to do machine learning, etc) and modify (calculate new columns, averages, etc) using Python.\n",
    "\n",
    "The dataframe will be accessible as the variable `df`.\n",
    "\n",
    "The final two cells are housekeeping tasks for convenience. We rename the `value` column (the name `value` was picked up from the CSV file header) and make sure the `timestamp` has the correct Pandas datatype (which allows advance time based operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Do this if the data is somewhere else\n",
    "# datadir = '/home/volkerh/Notebooks/biglearn_course'\n",
    "# g = glob.glob(\"%s/*.csv\" % datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = glob.glob(\"*DigiFab*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TempAir-DigiFab_20006414_2018-10-01 00_00_00_2018-11-30 23_59_59.csv']\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'value': 'air_temperature'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data / Descriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything, let us have a look at what we have in our dataframe.\n",
    "\n",
    "- The command `df.head(5)` show the first five rows of the dataframe.\n",
    "- The command `df.info()` shows general information (how many rows of data, what columns, what datatypes, etc)\n",
    "- The command `df.describe()` computes and shows basic descriptive statistics (percentiles, averages, and counts) for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe some code to look at the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to get a general overview of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to compute some descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analytics\n",
    "\n",
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some figures so we can actually see what the data looks like.\n",
    "\n",
    "Try making a figure that plots the variables in the dataframe vs. time.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- The simplest way to plot is to use `plt.plot(x_vector, y_vector)` (pretty much like Matlab)\n",
    "- To extract a particular column from your dataframe (which we have called `df`, do `df.NAME_OF_COLUMN`)\n",
    "- To add axis labels, you can use `plt.xlabel('SOME_TEXT')` or `plt.ylabel('SOME_TEXT')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms & Kernel Density Estimates [1D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may have found out that something interesting has happened to our sensor around **15 October** and **02 November**. Let's get back to the that.\n",
    "\n",
    "For now, a nice way of summarizing what a time series of measurements does is to plot the distribution of measurements. In other words, we ignore the time dependence and just have a look what kind of measurement values we observe.\n",
    "\n",
    "To easily plot some statistical summaries, we can use the `Seaborn` package. You way want to look at the `sns.distplot(vector_of_numbers)` function, which gives you a histogram as well as kernel density estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the statistical distribution of both variables in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms [2D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at how the two sensor measurements relate.\n",
    "\n",
    "The function `sns.jointplot(x_vector, y_vector)` will generate a plot of the joint and marginal distributions (as scatterplots and histograms, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the marginal and distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stare at the result a bit, you should be able to come up with some hypothesis of what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans have an outstanding visual processing capacity in their brains. Unfortuntaly, in the real world, we cannot have humans look at data streams from hundreds of sensors.\n",
    "\n",
    "So we would like to automated the procedure of finding clusters of different operating conditions for our sensors.\n",
    "\n",
    "This is called clustering.\n",
    "\n",
    "We now try a clustering algorithm that will attempt to find the two major clusters we have so easily found in the image above.\n",
    "\n",
    "To do clustering, we will be using the `Scikit-Learn` library. \n",
    "\n",
    "In `Scikit-Learn`, the programming interface for clustering algorithms (or most other algorithms, for that matter) is the same, i.e.\n",
    "\n",
    "1. You instantiate the right class (e.g., `algo = sklearn.cluster.SOME_ALGO()`)\n",
    "2. Fit the data (`algo.fit(matrix_with_data)`)\n",
    "3. Recover the labels via the `algo.labels_` attribute\n",
    "\n",
    "If you want to use, for example, DBSCAN, have a look at https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py.\n",
    "\n",
    "You can also look at the overview of clustering algorithms here: https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "Now, let's try to calculate and plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to compute clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot the clusters, we can use the basic scatterplot function `plt.scatter(x_value, y_values)`. There's also an argument `c` you can use to pass the color for each sample.\n",
    "\n",
    "See also https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html.\n",
    "\n",
    "If you can manage to use the labels (the number of the cluster) calculated by the clustering algorithm to make a scatterplot of the clusters, you should be able to do some interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to plot the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is (probably) same figure as above, except that we (you) have coloured each point according to the cluster the **ALGORITHM THINKS** the point is in.\n",
    "\n",
    "Depending on the clustering algorithm you selected (and its parameters), this may or may not have worked very well.\n",
    "\n",
    "This, unfortunately, the reality of applying machine learning in practice. During the first experiments, things usually don't go very smoothly and it takes experience (and persistence) to\n",
    "\n",
    "1. Select the most suitabel algorithms.\n",
    "2. Adjust it properly.\n",
    "\n",
    "This needs to be done with domain experts who know very well the proper operating regimes of their machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Anomaly Detection [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have found out that something happened with out sensor along the way.\n",
    "\n",
    "Ideally, we want to detect this automatically. Since the typical measurement range had a sharp transition, we can probably devise an simple (or not so simple) algorithm to detect and flag such jumps.\n",
    "\n",
    "For example, you could base this on deviations from an acceptable value range. The range could be based on moving averages.\n",
    "\n",
    "Of course, you can also use many other techniques to do outlier detection. There's a nice introduction here: https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "\n",
    "However, if you want to start with simple moving averages, note the following Pandas tricks:\n",
    "\n",
    "- You can calculate a rolling mean (over the past 128 samples) via `df[SOME_COLUMN_NAME].rolling(128).mean()`.\n",
    "- You can calculate a rolling standard deviation via `df[SOME_COLUMN_NAME].rolling(128).mean()`.\n",
    "- There are plenty more aggregation functions you can use, of course. See, for example, https://pandas.pydata.org/pandas-docs/stable/computation.html#window-functions\n",
    "- To return a subset of a dataframe `df`, you can do `df[(df.COLUMN>10) & (df.COLUMN<5)]`. See also https://pandas.pydata.org/pandas-docs/stable/indexing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting [Optional]\n",
    "\n",
    "Of course, you could also want to do forecasting of the measurements.\n",
    "\n",
    "A fairly straightforward implementation is available in the Facebook library `Prophet`, cf. https://facebook.github.io/prophet/\n",
    "\n",
    "But **do note that**, in general, you should bring along some sort of expert (or educate yourself to a sufficient level) that you know exactly what the model does. Otherwise you may not be able to spot whether it functions as designed or is trying to predict nonsense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
